{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, True, 1, True], [0, True, 3, True], [0, True, 0, True], [0, True, 0, True], [1, True, 0, True], [0, True, 2, True], [0, True, 1, True], [0, True, 1, True], [0, True, 1, True], [0, True, 0, True], [1, True, 1, True], [0, True, 0, True], [0, True, 2, True], [0, True, 1, True], [1, True, 2, True], [0, True, 1, True], [0, True, 2, True], [0, True, 1, True], [0, True, 0, True], [0, True, 0, True], [1, True, 0, True], [0, True, 1, True], [0, True, 2, True], [1, True, 0, True]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "def evaluate_article_responses(file_location, print_data = False):\n",
    "    # Load data from the given file location\n",
    "    data = pd.read_csv(file_location)\n",
    "    print(\"Size of the data: \", data.shape)\n",
    "    # Define the columns\n",
    "    columns = [\"False for true article\", \"All responses for true article\", \"False for false article\", \"All responses for false article\"]\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    grading = []\n",
    "    correct_grading = []\n",
    "\n",
    "    # Process each row in data\n",
    "    for index, result in data.iterrows():\n",
    "        if result[columns[1]] == 7:\n",
    "\n",
    "            grading.append(result[columns[0]] == 0)\n",
    "            correct_grading.append(True)\n",
    "        if result[columns[3]] == 7:\n",
    "            grading.append(result[columns[2]] == 0)\n",
    "            correct_grading.append(False)\n",
    "        if print_data:\n",
    "            print(result[columns[0]])\n",
    "            print(result[columns[2]])\n",
    "    # Convert to numpy arrays for computation\n",
    "    grading = np.array(grading)\n",
    "    correct_grading = np.array(correct_grading)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(correct_grading, grading)\n",
    "    precision = precision_score(correct_grading, grading)\n",
    "    f1 = f1_score(correct_grading, grading)\n",
    "    recall = recall_score(correct_grading, grading)\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "4.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "Accuracy: 0.77\n",
      "Recall: 0.70\n",
      "Precision: 0.82\n",
      "F1 Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"saved_results\"\n",
    "file_name = \"results_53_data_clean.csv\"\n",
    "path_string = os.path.join(folder_name, file_name)\n",
    "\n",
    "evaluate_article_responses(path_string, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "5\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "Accuracy: 0.65\n",
      "Recall: 0.42\n",
      "Precision: 0.88\n",
      "F1 Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"saved_results\"\n",
    "file_name = \"results_53_data_clean_mixtral8x7b.csv\"\n",
    "path_string = os.path.join(folder_name, file_name)\n",
    "\n",
    "evaluate_article_responses(path_string, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "Accuracy: 0.76\n",
      "Recall: 0.67\n",
      "Precision: 0.81\n",
      "F1 Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"saved_results\"\n",
    "file_name = \"results_53_data_clean_gemma2.csv\"\n",
    "path_string = os.path.join(folder_name, file_name)\n",
    "\n",
    "evaluate_article_responses(path_string, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "Accuracy: 0.82\n",
      "Recall: 0.90\n",
      "Precision: 0.78\n",
      "F1 Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"saved_results\"\n",
    "file_name = \"results_Llama3.1_one_by_one_generation_with_fact_also_one_by_one.csv\"\n",
    "path_string = os.path.join(folder_name, file_name)\n",
    "\n",
    "evaluate_article_responses(path_string, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data:  (141, 4)\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "Accuracy: 0.71\n",
      "Recall: 0.63\n",
      "Precision: 0.74\n",
      "F1 Score: 0.68\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"saved_results\"\n",
    "file_name = \"results_fake_article_one_by_one_generation_summarization_with_aggresive_prompts.csv\"\n",
    "path_string = os.path.join(folder_name, file_name)\n",
    "\n",
    "evaluate_article_responses(path_string, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data:  (70, 4)\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "Accuracy: 0.73\n",
      "Recall: 0.80\n",
      "Precision: 0.70\n",
      "F1 Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 7, 0, 7], [1, 7, 1, 7], [0, 7, 1, 7], [1, 7, 0, 7], [0, 7, 1, 7], [0, 7, 1, 7], [0, 7, 0, 7], [0, 7, 1, 7], [0, 7, 2, 7], [0, 7, 0, 7], [0, 7, 0, 7], [1, 7, 0, 7], [0, 7, 1, 7], [0, 7, 1, 7], [0, 7, 1, 7], [1, 7, 1, 7], [0, 7, 0, 6], [0, 7, 2, 7], [0, 7, 0, 7], [0, 7, 0, 7], [0, 7, 0, 7], [0, 7, 2, 7], [0, 7, 1, 7], [0, 7, 1, 7], [0, 7, 2, 7], [1, 7, 1, 7], [0, 7, 1, 7], [0, 7, 1, 7], [0, 7, 0, 7], [0, 7, 2, 7], [0, 7, 0, 7], [0, 7, 2, 7], [0, 7, 0, 7], [0, 7, 2, 7], [0, 7, 2, 7], [1, 7, 1, 7], [0, 7, 0, 7], [0, 7, 1, 7], [1, 7, 2, 7], [0, 7, 1, 7], [0, 7, 2, 7]]\n",
    "\n",
    "folder_name = \"saved_results\"\n",
    "file_name = \"results_Llama3.1_one_by_one_generation_with_fact_also_one_by_one_strict_labeling.csv\"\n",
    "path_string = os.path.join(folder_name, file_name)\n",
    "\n",
    "evaluate_article_responses(path_string, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Example true labels and predicted labels\n",
    "true_labels = [n for _ in range(len(prepared_data)//2) for n in [1, -1]]\n",
    "\n",
    "# Labels can be True (1), Neutral (0), and False (-1)\n",
    "y_true = np.array(true_labels)  # True labels\n",
    "y_pred = np.array(prepared_data)  # Predicted labels\n",
    "\n",
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "\n",
    "# Convert labels to binary for metrics \n",
    "# Here, we keep True as 1, Neutral as 0, and False as -1. \n",
    "# We will consider the True class as positive and Neutral/False as negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "# Compute F1 Score\n",
    "f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ES-testing",
   "language": "python",
   "name": "es-testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
